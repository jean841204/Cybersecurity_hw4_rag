"""
å‘é‡è³‡æ–™åº«å»ºç«‹è…³æœ¬
ä½¿ç”¨ FAISS + HuggingFace Embeddingsï¼ˆå®Œå…¨å…è²»ï¼‰
"""

import json
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

def load_books_data():
    """è¼‰å…¥æ›¸ç±è³‡æ–™"""
    with open('data/books_raw.json', 'r', encoding='utf-8') as f:
        books = json.load(f)
    print(f"ğŸ“š è¼‰å…¥ {len(books)} æœ¬æ›¸ç±")
    return books

def prepare_documents(books):
    """æº–å‚™æ–‡æª”æ ¼å¼"""
    documents = []

    for book in books:
        # çµ„åˆæˆå®Œæ•´æ–‡å­—
        text = f"""æ›¸åï¼š{book['title']}
ä½œè€…ï¼š{', '.join(book['authors'])}
å‡ºç‰ˆç¤¾ï¼š{book['publisher']}
å‡ºç‰ˆæ—¥æœŸï¼š{book['published_date']}
é¡åˆ¥ï¼š{', '.join(book['categories'])}
é æ•¸ï¼š{book['page_count']}

ç°¡ä»‹ï¼š
{book['description']}
"""

        # å»ºç«‹ Document ç‰©ä»¶
        doc = Document(
            page_content=text,
            metadata={
                'title': book['title'],
                'authors': ', '.join(book['authors']),
                'categories': ', '.join(book['categories']),
                'id': book['id'],
                'preview_link': book['preview_link'],
                'thumbnail': book['thumbnail']
            }
        )
        documents.append(doc)

    return documents

def build_vectordb():
    """å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«"""

    print("=" * 60)
    print("ğŸš€ é–‹å§‹å»ºç«‹å‘é‡è³‡æ–™åº«")
    print("=" * 60)

    # 1. è¼‰å…¥è³‡æ–™
    print("\n[1/4] ğŸ“š è¼‰å…¥æ›¸ç±è³‡æ–™...")
    books = load_books_data()

    # 2. æº–å‚™æ–‡æª”
    print("\n[2/4] ğŸ“ æº–å‚™æ–‡æª”...")
    documents = prepare_documents(books)
    print(f"æº–å‚™äº† {len(documents)} å€‹æ–‡æª”")

    # 3. æ–‡å­—åˆ‡å‰²
    print("\n[3/4] âœ‚ï¸  åˆ‡å‰²æ–‡å­—...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    print(f"åˆ‡å‰²æˆ {len(splits)} å€‹ç‰‡æ®µ")

    # 4. å»ºç«‹ Embeddingsï¼ˆæœ¬åœ°åŸ·è¡Œï¼Œå…è²»ï¼‰
    print("\n[4/4] ğŸ”„ å»ºç«‹å‘é‡è³‡æ–™åº«...")
    print("ä½¿ç”¨æ¨¡å‹ï¼šsentence-transformers/all-MiniLM-L6-v2")
    print("ï¼ˆæœ¬åœ°åŸ·è¡Œï¼Œå®Œå…¨å…è²»ï¼Œè¼•é‡å¿«é€Ÿï¼‰")

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«
    vectordb = FAISS.from_documents(
        documents=splits,
        embedding=embeddings
    )

    # 5. å„²å­˜
    print("\nğŸ’¾ å„²å­˜å‘é‡è³‡æ–™åº«...")
    save_path = "vectordb/faiss_index"
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    vectordb.save_local(save_path)

    print("\n" + "=" * 60)
    print("âœ… å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆï¼")
    print(f"ğŸ“ å„²å­˜ä½ç½®ï¼š{save_path}")
    print(f"ğŸ“Š æ–‡æª”æ•¸é‡ï¼š{len(splits)}")
    print("=" * 60)

    return vectordb

if __name__ == "__main__":
    build_vectordb()
