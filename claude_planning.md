å¥½çš„ï¼æˆ‘å¹«ä½ å¯«ä¸€å€‹å®Œæ•´çš„åŸ·è¡Œè¨ˆåŠƒçµ¦ Claude Codeï¼Œä¸¦ä¸”**å…¨éƒ¨ä½¿ç”¨å…è²»è³‡æº**ï¼

## ğŸ“‹ çµ¦ Claude Code çš„å°ˆæ¡ˆåŸ·è¡Œè¨ˆåŠƒ

```markdown
# å°ˆæ¡ˆï¼šå…è²» Google Books RAG æ¨è–¦ç³»çµ±

## å°ˆæ¡ˆç›®æ¨™
å»ºç«‹ä¸€å€‹æ›¸ç±æ¨è–¦ç³»çµ±ï¼Œä½¿ç”¨ RAG (Retrieval-Augmented Generation) æŠ€è¡“ï¼Œ
å®Œå…¨ä½¿ç”¨å…è²»è³‡æºï¼ŒåŒ…æ‹¬ï¼š
- Google Books APIï¼ˆå…è²»ï¼‰
- HuggingFace Inference APIï¼ˆå…è²»ï¼‰
- æœ¬åœ°å‘é‡è³‡æ–™åº«ï¼ˆFAISSï¼Œå…è²»ï¼‰
- Streamlitï¼ˆå…è²»éƒ¨ç½²ï¼‰

---

## æŠ€è¡“æ£§ï¼ˆå…¨å…è²»ï¼‰

### LLM é¸æ“‡
1. **HuggingFace Inference API**ï¼ˆæ¨è–¦ï¼‰
   - æ¨¡å‹ï¼š`google/flan-t5-xxl` æˆ– `mistralai/Mistral-7B-Instruct-v0.2`
   - å…è²»é¡åº¦ï¼šæœ‰ rate limit ä½†è¶³å¤ ä½¿ç”¨
   - æ”¯æ´ç¹é«”ä¸­æ–‡

2. **Google Gemini API**ï¼ˆå‚™é¸ï¼‰
   - æ¨¡å‹ï¼š`gemini-pro`
   - å…è²»é¡åº¦ï¼šæ¯åˆ†é˜ 60 requests
   - ä¸­æ–‡æ”¯æ´å¥½

### Embedding æ¨¡å‹
- HuggingFace `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- æœ¬åœ°åŸ·è¡Œï¼Œå®Œå…¨å…è²»
- æ”¯æ´ä¸­æ–‡

### å‘é‡è³‡æ–™åº«
- FAISSï¼ˆFacebook AI Similarity Searchï¼‰
- æœ¬åœ°å„²å­˜ï¼Œä¸éœ€è¦é›²ç«¯æœå‹™
- è¼•é‡ä¸”å¿«é€Ÿ

---

## å°ˆæ¡ˆçµæ§‹

```
books-rag-system/
â”œâ”€â”€ data_collection.py       # æ­¥é©Ÿ1ï¼šæ”¶é›† Google Books è³‡æ–™
â”œâ”€â”€ build_vectordb.py        # æ­¥é©Ÿ2ï¼šå»ºç«‹å‘é‡è³‡æ–™åº«
â”œâ”€â”€ app.py                   # æ­¥é©Ÿ3ï¼šStreamlit æ‡‰ç”¨ç¨‹å¼
â”œâ”€â”€ requirements.txt         # Python å¥—ä»¶
â”œâ”€â”€ .gitignore              
â”œâ”€â”€ README.md               
â”œâ”€â”€ data/
â”‚   â””â”€â”€ books_raw.json      # åŸå§‹æ›¸ç±è³‡æ–™
â””â”€â”€ vectordb/
    â””â”€â”€ faiss_index/        # FAISS å‘é‡è³‡æ–™åº«
```

---

## è©³ç´°å¯¦ä½œæ­¥é©Ÿ

### æ­¥é©Ÿ 0ï¼šç’°å¢ƒè¨­å®š

**requirements.txt**
```
streamlit==1.29.0
langchain==0.1.0
langchain-community==0.0.10
sentence-transformers==2.2.2
faiss-cpu==1.7.4
requests==2.31.0
google-generativeai==0.3.2
huggingface-hub==0.20.0
python-dotenv==1.0.0
```

**ç’°å¢ƒè®Šæ•¸è¨­å®šï¼ˆ.envï¼‰**
```
# é¸æ“‡ä¸€å€‹å³å¯

# æ–¹æ¡ˆ Aï¼šHuggingFaceï¼ˆæ¨è–¦ï¼‰
HUGGINGFACE_API_KEY=hf_your_key_here

# æ–¹æ¡ˆ Bï¼šGoogle Gemini
GOOGLE_API_KEY=your_google_api_key
```

**å¦‚ä½•å–å¾—å…è²» API Keyï¼š**

1. **HuggingFace Tokenï¼ˆæ¨è–¦ï¼‰**
   - è¨»å†Šï¼šhttps://huggingface.co/join
   - å‰å¾€ï¼šhttps://huggingface.co/settings/tokens
   - å»ºç«‹ "Read" tokenï¼ˆå…è²»ï¼‰
   - å…è²»é¡åº¦å……è¶³

2. **Google Gemini API**
   - å‰å¾€ï¼šhttps://makersuite.google.com/app/apikey
   - å»ºç«‹å…è²» API Key
   - æ¯æœˆ 60 requests/åˆ†é˜å…è²»

---

### æ­¥é©Ÿ 1ï¼šè³‡æ–™æ”¶é›†è…³æœ¬

**æª”æ¡ˆï¼šdata_collection.py**

```python
"""
Google Books è³‡æ–™æ”¶é›†è…³æœ¬
åŠŸèƒ½ï¼šå¾ Google Books API æ”¶é›†æ›¸ç±è³‡æ–™
"""

import requests
import json
import time
from pathlib import Path

def search_books(query, max_results=40, language='zh-TW'):
    """
    å¾ Google Books API æœå°‹æ›¸ç±
    
    Args:
        query: æœå°‹é—œéµå­—
        max_results: æœ€å¤šçµæœæ•¸
        language: èªè¨€é™åˆ¶
    
    Returns:
        æ›¸ç±åˆ—è¡¨
    """
    all_books = []
    
    for start_index in range(0, max_results, 40):
        url = "https://www.googleapis.com/books/v1/volumes"
        params = {
            'q': query,
            'langRestrict': language,
            'maxResults': min(40, max_results - start_index),
            'startIndex': start_index,
            'printType': 'books'
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            all_books.extend(data.get('items', []))
            print(f"å·²æ”¶é›† {len(all_books)} æœ¬æ›¸...")
            time.sleep(1)  # é¿å… rate limit
        except Exception as e:
            print(f"éŒ¯èª¤ï¼š{e}")
            continue
    
    return all_books

def extract_book_info(book):
    """æå–æ›¸ç±é‡è¦è³‡è¨Š"""
    volume_info = book.get('volumeInfo', {})
    
    return {
        'id': book.get('id'),
        'title': volume_info.get('title', 'ç„¡æ¨™é¡Œ'),
        'authors': volume_info.get('authors', ['æœªçŸ¥ä½œè€…']),
        'publisher': volume_info.get('publisher', 'æœªçŸ¥å‡ºç‰ˆç¤¾'),
        'published_date': volume_info.get('publishedDate', 'æœªçŸ¥'),
        'description': volume_info.get('description', 'ç„¡æè¿°'),
        'categories': volume_info.get('categories', ['æœªåˆ†é¡']),
        'page_count': volume_info.get('pageCount', 0),
        'language': volume_info.get('language', 'zh'),
        'preview_link': volume_info.get('previewLink', ''),
        'thumbnail': volume_info.get('imageLinks', {}).get('thumbnail', ''),
    }

def collect_books_data(categories=None, books_per_category=30):
    """
    æ”¶é›†å¤šå€‹é¡åˆ¥çš„æ›¸ç±
    
    Args:
        categories: é¡åˆ¥åˆ—è¡¨
        books_per_category: æ¯å€‹é¡åˆ¥æ”¶é›†æ•¸é‡
    """
    if categories is None:
        categories = [
            'å°èªª',
            'ç§‘å¹»',
            'æ¨ç†',
            'æ„›æƒ…',
            'æ­·å²',
            'ç§‘æ™®',
            'å•†æ¥­',
            'è‡ªæˆ‘æˆé•·',
            'å“²å­¸',
            'å¿ƒç†å­¸'
        ]
    
    all_books = []
    
    print(f"ğŸ“š é–‹å§‹æ”¶é›†æ›¸ç±è³‡æ–™...")
    print(f"é¡åˆ¥æ•¸é‡ï¼š{len(categories)}")
    print(f"æ¯é¡åˆ¥ï¼š{books_per_category} æœ¬")
    print("-" * 50)
    
    for i, category in enumerate(categories, 1):
        print(f"\n[{i}/{len(categories)}] æ”¶é›†ã€Œ{category}ã€é¡åˆ¥...")
        books = search_books(category, max_results=books_per_category)
        processed_books = [extract_book_info(book) for book in books]
        all_books.extend(processed_books)
        time.sleep(1)
    
    # å»é‡ï¼ˆæ ¹æ“š IDï¼‰
    unique_books = {book['id']: book for book in all_books}.values()
    unique_books = list(unique_books)
    
    # å„²å­˜è³‡æ–™
    Path('data').mkdir(exist_ok=True)
    output_file = 'data/books_raw.json'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(unique_books, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 50)
    print(f"âœ… æ”¶é›†å®Œæˆï¼")
    print(f"ğŸ“Š ç¸½å…±æ”¶é›†ï¼š{len(unique_books)} æœ¬æ›¸ç±")
    print(f"ğŸ’¾ å„²å­˜ä½ç½®ï¼š{output_file}")
    print("=" * 50)
    
    return unique_books

if __name__ == "__main__":
    books = collect_books_data()
```

---

### æ­¥é©Ÿ 2ï¼šå»ºç«‹å‘é‡è³‡æ–™åº«

**æª”æ¡ˆï¼šbuild_vectordb.py**

```python
"""
å‘é‡è³‡æ–™åº«å»ºç«‹è…³æœ¬
ä½¿ç”¨ FAISS + HuggingFace Embeddingsï¼ˆå®Œå…¨å…è²»ï¼‰
"""

import json
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

def load_books_data():
    """è¼‰å…¥æ›¸ç±è³‡æ–™"""
    with open('data/books_raw.json', 'r', encoding='utf-8') as f:
        books = json.load(f)
    print(f"ğŸ“š è¼‰å…¥ {len(books)} æœ¬æ›¸ç±")
    return books

def prepare_documents(books):
    """æº–å‚™æ–‡æª”æ ¼å¼"""
    documents = []
    
    for book in books:
        # çµ„åˆæˆå®Œæ•´æ–‡å­—
        text = f"""æ›¸åï¼š{book['title']}
ä½œè€…ï¼š{', '.join(book['authors'])}
å‡ºç‰ˆç¤¾ï¼š{book['publisher']}
å‡ºç‰ˆæ—¥æœŸï¼š{book['published_date']}
é¡åˆ¥ï¼š{', '.join(book['categories'])}
é æ•¸ï¼š{book['page_count']}

ç°¡ä»‹ï¼š
{book['description']}
"""
        
        # å»ºç«‹ Document ç‰©ä»¶
        doc = Document(
            page_content=text,
            metadata={
                'title': book['title'],
                'authors': ', '.join(book['authors']),
                'categories': ', '.join(book['categories']),
                'id': book['id'],
                'preview_link': book['preview_link'],
                'thumbnail': book['thumbnail']
            }
        )
        documents.append(doc)
    
    return documents

def build_vectordb():
    """å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«"""
    
    print("=" * 60)
    print("ğŸš€ é–‹å§‹å»ºç«‹å‘é‡è³‡æ–™åº«")
    print("=" * 60)
    
    # 1. è¼‰å…¥è³‡æ–™
    print("\n[1/4] ğŸ“š è¼‰å…¥æ›¸ç±è³‡æ–™...")
    books = load_books_data()
    
    # 2. æº–å‚™æ–‡æª”
    print("\n[2/4] ğŸ“ æº–å‚™æ–‡æª”...")
    documents = prepare_documents(books)
    print(f"æº–å‚™äº† {len(documents)} å€‹æ–‡æª”")
    
    # 3. æ–‡å­—åˆ‡å‰²
    print("\n[3/4] âœ‚ï¸  åˆ‡å‰²æ–‡å­—...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    print(f"åˆ‡å‰²æˆ {len(splits)} å€‹ç‰‡æ®µ")
    
    # 4. å»ºç«‹ Embeddingsï¼ˆæœ¬åœ°åŸ·è¡Œï¼Œå…è²»ï¼‰
    print("\n[4/4] ğŸ”„ å»ºç«‹å‘é‡è³‡æ–™åº«...")
    print("ä½¿ç”¨æ¨¡å‹ï¼šsentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    print("ï¼ˆæœ¬åœ°åŸ·è¡Œï¼Œå®Œå…¨å…è²»ï¼Œæ”¯æ´ä¸­æ–‡ï¼‰")
    
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«
    vectordb = FAISS.from_documents(
        documents=splits,
        embedding=embeddings
    )
    
    # 5. å„²å­˜
    print("\nğŸ’¾ å„²å­˜å‘é‡è³‡æ–™åº«...")
    save_path = "vectordb/faiss_index"
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    vectordb.save_local(save_path)
    
    print("\n" + "=" * 60)
    print("âœ… å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆï¼")
    print(f"ğŸ“ å„²å­˜ä½ç½®ï¼š{save_path}")
    print(f"ğŸ“Š æ–‡æª”æ•¸é‡ï¼š{len(splits)}")
    print("=" * 60)
    
    return vectordb

if __name__ == "__main__":
    build_vectordb()
```

---

### æ­¥é©Ÿ 3ï¼šStreamlit æ‡‰ç”¨ç¨‹å¼

**æª”æ¡ˆï¼šapp.py**

```python
"""
Google Books RAG æ¨è–¦ç³»çµ±
ä½¿ç”¨å…è²»è³‡æºï¼šFAISS + HuggingFace/Gemini
"""

import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv

load_dotenv()

# ==================== é…ç½®é¸é … ====================

# é¸æ“‡ä½¿ç”¨å“ªå€‹ LLMï¼ˆäºŒé¸ä¸€ï¼‰
USE_LLM = "huggingface"  # æˆ– "gemini"

# ==================== LLM è¨­å®š ====================

def get_llm():
    """æ ¹æ“šé…ç½®è¿”å›å°æ‡‰çš„ LLM"""
    
    if USE_LLM == "huggingface":
        from langchain_community.llms import HuggingFaceHub
        
        api_key = os.getenv("HUGGINGFACE_API_KEY") or st.secrets.get("HUGGINGFACE_API_KEY")
        
        llm = HuggingFaceHub(
            repo_id="mistralai/Mistral-7B-Instruct-v0.2",  # æˆ– "google/flan-t5-xxl"
            huggingfacehub_api_token=api_key,
            model_kwargs={
                "temperature": 0.7,
                "max_length": 512
            }
        )
        return llm
    
    elif USE_LLM == "gemini":
        from langchain_google_genai import ChatGoogleGenerativeAI
        
        api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
        
        llm = ChatGoogleGenerativeAI(
            model="gemini-pro",
            google_api_key=api_key,
            temperature=0.7
        )
        return llm

# ==================== å¿«å–å‡½æ•¸ ====================

@st.cache_resource
def load_vectordb():
    """è¼‰å…¥å‘é‡è³‡æ–™åº«ï¼ˆåªè¼‰å…¥ä¸€æ¬¡ï¼‰"""
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    vectordb = FAISS.load_local(
        "vectordb/faiss_index",
        embeddings,
        allow_dangerous_deserialization=True
    )
    
    return vectordb

@st.cache_resource
def create_qa_chain(_vectordb):
    """å»ºç«‹å•ç­”éˆ"""
    
    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ›¸ç±æ¨è–¦åŠ©æ‰‹ã€‚æ ¹æ“šä»¥ä¸‹æ›¸ç±è³‡è¨Šå›ç­”å•é¡Œã€‚

ç›¸é—œæ›¸ç±è³‡è¨Šï¼š
{context}

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”ï¼š
1. æ¨è–¦ 2-3 æœ¬æœ€ç›¸é—œçš„æ›¸ç±
2. èªªæ˜æ¨è–¦ç†ç”±
3. æä¾›æ›¸åã€ä½œè€…å’Œç°¡çŸ­ä»‹ç´¹

å›ç­”ï¼š"""

    PROMPT = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    
    llm = get_llm()
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=_vectordb.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    return qa_chain

# ==================== Streamlit UI ====================

def main():
    st.set_page_config(
        page_title="ğŸ“š AI æ›¸ç±æ¨è–¦åŠ©æ‰‹",
        page_icon="ğŸ“š",
        layout="wide"
    )
    
    # æ¨™é¡Œ
    st.title("ğŸ“š AI æ›¸ç±æ¨è–¦åŠ©æ‰‹")
    st.markdown("åŸºæ–¼ Google Books è³‡æ–™ï¼Œä½¿ç”¨ RAG æŠ€è¡“æ¨è–¦å¥½æ›¸ | ğŸ†“ å®Œå…¨å…è²»")
    
    # é¡¯ç¤ºä½¿ç”¨çš„æŠ€è¡“
    st.caption(f"ğŸ’¡ ä½¿ç”¨æŠ€è¡“ï¼šFAISS + {USE_LLM.upper()} | æœ¬åœ° Embeddings")
    
    # è¼‰å…¥è³‡æº
    try:
        with st.spinner("è¼‰å…¥æ›¸ç±è³‡æ–™åº«..."):
            vectordb = load_vectordb()
            qa_chain = create_qa_chain(vectordb)
        st.success("âœ… è³‡æ–™åº«è¼‰å…¥å®Œæˆï¼")
    except Exception as e:
        st.error(f"âŒ è¼‰å…¥å¤±æ•—ï¼š{e}")
        st.info("è«‹ç¢ºèªï¼š\n1. å·²åŸ·è¡Œ build_vectordb.py\n2. API Key å·²è¨­å®š")
        return
    
    # å´é‚Šæ¬„
    with st.sidebar:
        st.header("ğŸ’¡ ä½¿ç”¨èªªæ˜")
        st.markdown("""
        è¼¸å…¥æ‚¨çš„å•é¡Œï¼ŒAI æœƒæ¨è–¦ç›¸é—œæ›¸ç±
        
        **ç¯„ä¾‹å•é¡Œï¼š**
        - æ¨è–¦ç§‘å¹»å°èªª
        - æœ‰ä»€éº¼å•†æ¥­æ›¸ç±ï¼Ÿ
        - é©åˆåˆå­¸è€…çš„å¿ƒç†å­¸æ›¸
        - æ¨è–¦ç¶“å…¸æ–‡å­¸ä½œå“
        """)
        
        st.divider()
        
        st.header("âš™ï¸ ç³»çµ±è³‡è¨Š")
        st.write(f"**LLM**ï¼š{USE_LLM}")
        st.write(f"**å‘é‡è³‡æ–™åº«**ï¼šFAISS")
        st.write(f"**Embeddings**ï¼šæœ¬åœ°æ¨¡å‹")
        
        # é¡¯ç¤ºçµ±è¨ˆ
        if vectordb:
            st.metric("è³‡æ–™åº«æ–‡æª”æ•¸", vectordb.index.ntotal)
    
    # ä¸»è¦å€åŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ” æå‡ºå•é¡Œ")
        question = st.text_input(
            "è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š",
            placeholder="ä¾‹å¦‚ï¼šæ¨è–¦é©åˆæ–°æ‰‹çš„ç§‘å¹»å°èªª",
            label_visibility="collapsed"
        )
    
    with col2:
        st.write("")  # ç©ºç™½å°é½Š
        submit = st.button("ğŸš€ ç²å–æ¨è–¦", type="primary", use_container_width=True)
    
    # ç¯„ä¾‹å•é¡ŒæŒ‰éˆ•
    st.markdown("**å¿«é€Ÿç¯„ä¾‹ï¼š**")
    col1, col2, col3, col4 = st.columns(4)
    
    example_questions = {
        "ç§‘å¹»å°èªª": "æ¨è–¦ç¶“å…¸ç§‘å¹»å°èªª",
        "å•†æ¥­ç†è²¡": "æœ‰ä»€éº¼å•†æ¥­æˆ–ç†è²¡æ›¸ï¼Ÿ",
        "å¿ƒç†å­¸": "æ¨è–¦å¿ƒç†å­¸ç›¸é—œæ›¸ç±",
        "æ­·å²": "æœ‰ä»€éº¼æ­·å²é¡çš„å¥½æ›¸ï¼Ÿ"
    }
    
    for col, (label, q) in zip([col1, col2, col3, col4], example_questions.items()):
        with col:
            if st.button(label, use_container_width=True):
                question = q
                submit = True
    
    # è™•ç†å•ç­”
    if submit and question:
        with st.spinner("ğŸ¤” æ€è€ƒä¸­..."):
            try:
                result = qa_chain({"query": question})
                
                # é¡¯ç¤ºç­”æ¡ˆ
                st.markdown("### ğŸ’¬ æ¨è–¦çµæœ")
                st.write(result['result'])
                
                # é¡¯ç¤ºåƒè€ƒæ›¸ç±
                st.markdown("### ğŸ“š åƒè€ƒæ›¸ç±")
                
                for i, doc in enumerate(result['source_documents'][:3], 1):
                    with st.expander(f"ğŸ“– æ›¸ç± {i}ï¼š{doc.metadata.get('title', 'æœªçŸ¥')}"):
                        col_a, col_b = st.columns([1, 3])
                        
                        with col_a:
                            if doc.metadata.get('thumbnail'):
                                st.image(doc.metadata['thumbnail'], width=120)
                        
                        with col_b:
                            st.write(f"**ä½œè€…**ï¼š{doc.metadata.get('authors', 'æœªçŸ¥')}")
                            st.write(f"**é¡åˆ¥**ï¼š{doc.metadata.get('categories', 'æœªçŸ¥')}")
                            if doc.metadata.get('preview_link'):
                                st.markdown(f"[ğŸ“± é è¦½é€£çµ]({doc.metadata['preview_link']})")
                
            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                st.info("è«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢ºè¨­å®š")
    
    elif submit and not question:
        st.warning("âš ï¸ è«‹è¼¸å…¥å•é¡Œï¼")

if __name__ == "__main__":
    main()
```

---

## åŸ·è¡Œé †åº

### æœ¬åœ°æ¸¬è©¦

```bash
# 1. å®‰è£å¥—ä»¶
pip install -r requirements.txt

# 2. è¨­å®š API Keyï¼ˆæ“‡ä¸€ï¼‰
# å»ºç«‹ .env æª”æ¡ˆï¼ŒåŠ å…¥ï¼š
# HUGGINGFACE_API_KEY=hf_xxxx
# æˆ–
# GOOGLE_API_KEY=xxxx

# 3. æ”¶é›†è³‡æ–™
python data_collection.py

# 4. å»ºç«‹å‘é‡è³‡æ–™åº«ï¼ˆåªéœ€åŸ·è¡Œä¸€æ¬¡ï¼Œç´„éœ€ 5-10 åˆ†é˜ï¼‰
python build_vectordb.py

# 5. å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼
streamlit run app.py
```

### éƒ¨ç½²åˆ° Streamlit Cloud

1. å°‡å°ˆæ¡ˆæ¨åˆ° GitHubï¼ˆä¸è¦åŒ…å« .envï¼‰
2. åœ¨ Streamlit Cloud å»ºç«‹æ–°æ‡‰ç”¨
3. åœ¨ Settings â†’ Secrets åŠ å…¥ API Keyï¼š
```toml
HUGGINGFACE_API_KEY = "hf_your_key"
# æˆ–
GOOGLE_API_KEY = "your_key"
```

---

## æ³¨æ„äº‹é …

1. **ç¬¬ä¸€æ¬¡åŸ·è¡Œ build_vectordb.py æœƒä¸‹è¼‰æ¨¡å‹**
   - ç´„ 400MB
   - åªéœ€ä¸‹è¼‰ä¸€æ¬¡
   - ä¹‹å¾Œéƒ½æ˜¯æœ¬åœ°åŸ·è¡Œ

2. **HuggingFace å…è²»é™åˆ¶**
   - æ¯å°æ™‚ç´„ 1000 æ¬¡è«‹æ±‚
   - è¶³å¤ å€‹äººä½¿ç”¨æˆ– Demo

3. **æª”æ¡ˆå¤§å°**
   - vectordb/ è³‡æ–™å¤¾ç´„ 100-200MB
   - å¯ä»¥æ¨åˆ° GitHub
   - æˆ–åœ¨éƒ¨ç½²æ™‚é‡æ–°å»ºç«‹

4. **æ•ˆèƒ½**
   - è¼‰å…¥è³‡æ–™åº«ï¼š3-5 ç§’
   - å–®æ¬¡å•ç­”ï¼š5-10 ç§’
   - Streamlit Cloud ä¸æœƒ timeout

---

## é€²éšåŠŸèƒ½å»ºè­°

1. **åŠ å…¥æ›¸ç±å°é¢é¡¯ç¤º**
2. **åŒ¯å‡ºæ¨è–¦æ¸…å–®ç‚º PDF**
3. **åŠ å…¥ä½¿ç”¨è€…è©•åˆ†åŠŸèƒ½**
4. **å¤šèªè¨€æ”¯æ´ï¼ˆä¸­è‹±åˆ‡æ›ï¼‰**
5. **é¡¯ç¤ºç›¸ä¼¼æ›¸ç±æ¨è–¦**

---

## ç–‘é›£æ’è§£

### Q: HuggingFace æ¨¡å‹ä¸‹è¼‰å¤ªæ…¢ï¼Ÿ
A: å¯ä»¥å…ˆåœ¨æœ¬åœ°ä¸‹è¼‰ï¼Œå†éƒ¨ç½²

### Q: Streamlit Cloud ä¸Šå‚³å¤±æ•—ï¼Ÿ
A: vectordb å¤ªå¤§ï¼Œå¯ä»¥åœ¨ GitHub Actions è‡ªå‹•å»ºç«‹

### Q: å›ç­”å“è³ªä¸ä½³ï¼Ÿ
A: å¯ä»¥èª¿æ•´ Prompt æˆ–æ›ç”¨ Gemini

---

è«‹æŒ‰ç…§é€™å€‹è¨ˆåŠƒåŸ·è¡Œï¼Œæœ‰ä»»ä½•å•é¡Œéš¨æ™‚å›å ±ï¼
```

---

## ğŸ“ é¡å¤–è£œå……ï¼šå¦‚ä½•å–å¾—å…è²» API Key

### HuggingFace Tokenï¼ˆæ¨è–¦ï¼ï¼‰

1. å‰å¾€ https://huggingface.co/join è¨»å†Š
2. ç™»å…¥å¾Œåˆ° https://huggingface.co/settings/tokens
3. é»é¸ "New token"
4. Token é¡å‹é¸ "Read"
5. è¤‡è£½ Tokenï¼ˆæ ¼å¼ï¼š`hf_xxxxxx`ï¼‰

**å„ªé»ï¼š**
- å®Œå…¨å…è²»
- é¡åº¦å……è¶³
- æ”¯æ´å¤šç¨®æ¨¡å‹

### Google Gemini API

1. å‰å¾€ https://makersuite.google.com/app/apikey
2. ç™»å…¥ Google å¸³è™Ÿ
3. é»é¸ "Create API Key"
4. è¤‡è£½ API Key

**å„ªé»ï¼š**
- æ¯åˆ†é˜ 60 requests
- ä¸­æ–‡è¡¨ç¾å¥½
- å›æ‡‰é€Ÿåº¦å¿«

---

é€™æ¨£ä½ å°±æœ‰ä¸€å€‹**å®Œå…¨å…è²»**çš„ RAG ç³»çµ±äº†ï¼éœ€è¦æˆ‘è§£é‡‹å“ªå€‹éƒ¨åˆ†å—ï¼Ÿ